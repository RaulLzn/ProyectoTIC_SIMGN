"""
Script de prueba para verificar cu√°ntos archivos Excel encuentra el scraper
"""
import requests
from bs4 import BeautifulSoup
import re

MINENERGIA_URL = "https://www.minenergia.gov.co/es/misional/hidrocarburos/funcionamiento-del-sector/gas-natural/"

def test_scraper():
    print("="*70)
    print("üîç ESCANEANDO P√ÅGINA DE MINENERGIA")
    print("="*70)
    print(f"URL: {MINENERGIA_URL}\n")
    
    try:
        response = requests.get(MINENERGIA_URL, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Buscar TODOS los enlaces
        all_links = soup.find_all('a', href=True)
        print(f"‚úì Total de enlaces encontrados: {len(all_links)}\n")
        
        # Filtrar archivos Excel
        excel_files = []
        for link in all_links:
            href = link['href']
            text = link.get_text(strip=True)
            
            # Buscar archivos Excel
            if any(ext in href.lower() for ext in ['.xlsx', '.xlsm', '.xls']):
                # Filtrar por palabras clave relacionadas con producci√≥n
                if any(keyword in (href + text).lower() for keyword in 
                       ['declaracion', 'produccion', 'soporte', 'magnetico']):
                    
                    # Construir URL completa
                    if not href.startswith('http'):
                        full_url = "https://www.minenergia.gov.co" + href
                    else:
                        full_url = href
                    
                    # Extraer per√≠odo
                    period_match = re.search(r'(20\d{2})\s*-?\s*(20\d{2})?', text + href)
                    period = period_match.group(0) if period_match else 'Unknown'
                    
                    excel_files.append({
                        'url': full_url,
                        'text': text or href.split('/')[-1],
                        'period': period
                    })
        
        # Eliminar duplicados
        unique_urls = {}
        for file in excel_files:
            url = file['url']
            if url not in unique_urls:
                unique_urls[url] = file
        
        print(f"üìä ARCHIVOS EXCEL ENCONTRADOS: {len(unique_urls)}")
        print("="*70)
        
        # Mostrar todos los archivos encontrados
        for i, (url, info) in enumerate(sorted(unique_urls.items()), 1):
            print(f"\n{i}. Per√≠odo: {info['period']}")
            print(f"   Descripci√≥n: {info['text'][:60]}")
            print(f"   URL: {url[:80]}...")
        
        print("\n" + "="*70)
        print(f"‚úÖ TOTAL: {len(unique_urls)} archivos √∫nicos de producci√≥n")
        print("="*70)
        
        return len(unique_urls)
        
    except Exception as e:
        print(f"‚ùå ERROR: {e}")
        return 0

if __name__ == "__main__":
    count = test_scraper()
    
    if count > 0:
        print(f"\nüí° El scraper mejorado puede procesar {count} archivos")
        print("   Esto es mucho m√°s que el archivo √∫nico anterior!")
    else:
        print("\n‚ö†Ô∏è  No se encontraron archivos")
